#### 关系型和非关系型数据库的区别（低频）

* 关系型数据库的优点
  1. 容易理解，因为它采用了关系模型来组织数据。
  2. 严格的事务一致性保证。
  3. 数据更新的开销比较小。
  4. 支持复杂查询（带where子句的查询）
* 非关系型数据库的优点
  1. 不需要经过sql层的解析，读写效率高。
  2. 基于键值对，数据的扩展性很好。
  3. 可以支持多种类型数据的存储，如图片，文档等等。

https://xmmarlowe.github.io/2021/04/25/%E6%95%B0%E6%8D%AE%E5%BA%93/%E8%81%8A%E8%81%8A%E5%AF%B9%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%92%8C%E9%9D%9E%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E7%90%86%E8%A7%A3/

#### 什么是非关系型数据库（低频）

非关系型数据库也叫nosql，采用键值对的形式进行存储。它的读写性能很高，易于扩展。例如Redis,Mongodb,hbase等等。

适合使用非关系型数据库的场景：

* 日志系统
* 地理位置存储
* 数据量巨大
* 高可用

#### 为什么使用索引？

- 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。
- 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。
- 帮助服务器避免排序和临时表
- 将随机IO变为顺序IO。
- 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。

**缺点：**

- 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行insert、update和delete。因为更新表时，不仅要保存数据，还要保存一下索引文件。
- 建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会增长很快。
- 索引只是提高效率的一个因素，如果有大数据量的表，就需要花时间研究建立最优秀的索引，或优化查询语句。

https://www.cnblogs.com/wezheng/p/8399305.html

哪些列适合建索引？

* 经常搜索的列上建索引
* 作为主键的列上要建索引
* 经常需要连接（where子句）的列上
* 经常需要排序的列
* 经常需要范围查找的列

哪些列不适合建索引？

* 很少查询的列
* 更新很频繁的列
* 数据值的取值比较少的列（比如性别）

#### 索引最左前缀/最左匹配

在MySQL建立联合索引时会遵守最左前缀匹配原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。

假如我们对a b c三个字段建立了联合索引，在联合索引中，从最左边的字段开始，任何连续的索引都能匹配上，当遇到范围查询的时候停止。比如对于联合索引index(a,b,c),能匹配a,ab,abc三组索引。并且对查询时字段的顺序没有限制，也就是a,b,c; b,a,c; c,a,b; c,b,a都可以匹配。

####  说一下 MySQL 执行一条查询语句的内部执行过程？

<img src="https://camo.githubusercontent.com/a72810e14c3de7fb0c305019f23b95d79c929cede060ce9633943495ad0ec160/68747470733a2f2f63646e2e6a7364656c6976722e6e65742f67682f666f7274686573706164612f6d65646961496d6167653240322e352f3230323130342f6d7973716c2d342e706e67" alt="img" style="zoom: 80%;" />

* 连接器：客户端先通过连接器连接到 MySQL 服务器。
* 查询缓存：连接器权限验证通过之后，先查询是否有查询缓存，如果有缓存（之前执行过此语句）则直接返回缓存数据，如果没有缓存则进入分析器。
* 分析器：分析器会对查询语句进行语法分析和词法分析，判断 SQL 语法是否正确，如果查询语法错误会直接返回给客户端错误信息，如果语法正确则进入优化器。
* 优化器：优化器是对查询语句进行优化处理，例如一个表里面有多个索引，优化器会判别哪个索引性能更好。
* 执行器：优化器执行完就进入执行器，执行器就开始对存储引擎执行语句并进行查询比对了，直到查询到满足条件的所有数据，然后进行返回。

#### 一条更新语句在MySQL是怎么执行的

- 连接验证及解析：客户端通过连接器连接到 MySQL 服务器。分析器会先做词法分析，识别出关键字update，表名等等；之后还会做语法分析，判断输入的语句是否符合MySQL语法。优化器生成执行计划，选择使用哪个索引，然后执行器会调用存储引擎的接口去执行语句。
- undo log：innodb 引擎首先开启事务，获得一个事务ID(是一直递增的)，写undo log （是逻辑日志，记录的是SQL语句），记录上一个版本数据，并更新记录的回滚指针和事务ID
- 从索引中查找数据并更新：根据索引去B+树中找到要更新的数据，
  - 如果数据页在内存中
    - 如果索引是普通索引，则直接更新内存中的数据页。
    - 如果索引是唯一索引，判断更新后是否会数据冲突(不能破坏索引的唯一性)，不会的话就更新内存中的数据页。
  - 如果数据页不在内存中
    - 如果索引是普通索引，将对数据页的更新操作缓存到change buffer中，暂时不更新到磁盘，change buffer会在空闲时异步更新到磁盘。
    - 索引是唯一索引，则必须把数据页从磁盘加载到内存，然后判断更新后是否会数据冲突，不会的话就更新数据页。
- redo log：将更新操作记录到redo log，刷新redo log到磁盘中，并将redo log状态设置为prepare
- bin log：将执行的SQL写入到bin log，刷新bin log到磁盘中
- 调用存储引擎的提交事务接口，把状态设置为commit。

<img src="https://gsmtoday.github.io/2019/02/08/how-update-executes-in-mysql/update%20process.png" alt="MySQL更新语句执行过程" style="zoom: 50%;" />

**redo log、 binlog 的 差异**

1. redo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。
2. redo log 是物理日志，记录的是“在某个数据页做了什么修改”；而 binlog 是逻辑日志，记录的是语句的原始逻辑。比如 `update T set c=c+1 where ID=2;`这条SQL，redo log 中记录的是 ：`xx页号，xx偏移量的数据修改为xxx；`binlog 中记录的是：`id = 2 这一行的 c 字段 +1`。
3. redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。
4. redo log 用于崩溃恢复，binlog 用于主从复制和数据恢复。

**宕机后的数据恢复规则**

1、如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交；
2、如果 redo log 里面的事物只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：如果是，则提交事务；否则，回滚事务。

https://www.socoinn.com/blog/sy1995/614

https://learnku.com/articles/49614

https://gsmtoday.github.io/2019/02/08/how-update-executes-in-mysql/

#### 数据库的索引类型

数据库的索引类型分为逻辑分类和物理分类

逻辑分类：

* 主键索引 当关系表中定义主键时会自动创建主键索引。一个表只能建立一个主键索引，要求主键中的每个值都**唯一**，即不可重复，也**不能有空值**。
* 唯一索引 在表上一个或者多个字段组合建立的索引，数据列**不能有重复**，**可以有空值**，一张表可以有**多个唯一索引**。

```mysql
CREATE UNIQUE INDEX indexName ON table(column(length))
ALTER TABLE table_name ADD [UNIQUE|FULLLTEXT] INDEX index_name (column(length))
```

* 普通索引 在表上一个或者多个字段组合建立的索引，可以重复可以为空值，一张表可以有多个普通索引。

```mysql
CREATE INDEX index_name ON table(column(length))
```

* 全文索引 可以加快模糊查询，不常用

```mysql
CREATE FULLTEXT INDEX index_content ON article(content)
```

- 前缀索引 对字符类型字段的前几个字符或对二进制类型字段的前几个bytes建立的索引，而不是在整个字段上建索引。前缀索引可以建立在类型为char、varchar、binary、varbinary的列上，可以大大减少索引占用的存储空间，也能提升索引的查询效率。

物理分类：

* **聚集索引**（聚簇索引） 数据在物理存储中的顺序跟索引中数据的逻辑顺序相同，比如以ID建立聚集索引，数据库中id从小到大排列，那么物理存储中该数据的内存地址值也按照从小到大存储。一般是表中的主键索引，如果没有主键索引就会以第一个非空的唯一索引作为聚集索引。**一张表只能有一个聚集索引**。
* **非聚集索引** 数据在物理存储中的顺序跟索引中数据的逻辑顺序不同（独立数据行的结构，每个键值项都有指向包含该键值的数据行的指针）。非聚集索引因为无法定位数据所在的行，所以需要扫描**两遍索引树**。第一遍扫描非聚集索引的索引树，确定该数据的**主键ID**，然后到主键索引（聚集索引）中寻找**相应的数据**。

**聚簇索引**的每个叶子节点存储了一行完整的表数据。

<img src="https://segmentfault.com/img/remote/1460000037688814/view" alt="preview" style="zoom: 50%;" />

优点：查询速度快，因为一旦具有第一个索引值的记录被找到，具有连续索引值的记录也一定物理的紧跟其后。

缺点：对表进行修改速度较慢，这是为了保持表中的记录的物理顺序与索引的顺序一致，而把记录插入到数据页的相应位置，必须在数据页中进行数据重排，降低了执行速度。

建议使用聚簇索引的场合：

- 某列包含了小数目的不同值
- 排序和范围查找

**非聚簇索引**的叶子节点不存储表数据，存放的是表数据的地址。

<img src="https://segmentfault.com/img/remote/1460000037688815/view" alt="preview" style="zoom: 50%;" />

聚簇索引以外的其他索引叫做二级索引，也叫辅助索引。

二级索引的叶子节点并不存储一行完整的表数据，而是存储了聚簇索引所在列的值。

<img src="https://segmentfault.com/img/remote/1460000037688816/view" alt="preview" style="zoom:50%;" />

由于二级索引的叶子节点不存储完整的表数据，索引当通过二级索引查询到聚簇索引列值后，还需要回到聚簇索引也就是表数据本身进一步获取数据。

<img src="https://segmentfault.com/img/remote/1460000037688817/view" alt="preview" style="zoom: 67%;" />

辅助索引使用主键作为"指针", 而不是使用地址值作为指针的好处是， 减少了当出现行移动或者数据页分裂时,辅助索引的维护工作。InnoDB 在移动行时无须更新辅助索引中的这个"指针"。 也就是说行的位置会随着数据库里数据的修改而发生变化， 使用聚簇索引就可以保证不管这个主键 B+树的节点如何变化， 辅助索引树都不受影响。

#### 都知道数据库索引采用B+树而不是B树，原因也有很多，主要原因是什么？

1. B+树是一种特殊的平衡多路树，是B树的优化改进版本，它把所有的数据都存放在叶节点上，中间节点保存的是索引。这样一来相对于B树来说，**减少了数据对中间节点的空间占用**，**使得中间节点可以存放更多的指针**，**使得树变得更矮，深度更小，从而减少查询的磁盘IO次数**，提高查询效率。

2. 由于叶节点之间有指针连接，所以可以进行范围查询，方便区间访问。

而红黑树是二叉的，它的深度相对B+树来说更大，更大的深度意味着查找次数更多，更频繁的磁盘IO，所以红黑树更适合在内存中进行查找。

#### 文件索引和数据库索引为什么使用B+树?

文件与数据库都是需要较大的存储，也就是说，它们都不可能全部存储在内存中，故需要存储到磁盘上。而所谓索引，则为了数据的快速定位与查找，那么索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数，因此B+树相比B树更为合适。数据库系统巧妙利用了局部性原理与磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入，而红黑树这种结构，高度明显要深的多，并且由于逻辑上很近的节点(父子)物理上可能很远，无法利用局部性。

B+树支持范围查询非常方便，而B树只能中序遍历所有节点。

B+树查找效率更稳定，数据都存储在叶节点，而B树有可能在中间节点找到数据，稳定性不够。

利用Hash需要把数据全部加载到内存中，如果数据量大，是一件很消耗内存的事，而B+树允许数据分批加载，由此减少内存消耗。

对于唯一查找（查找一个值），Hash确实更快，但数据库中经常查询多条数据，这时候由于B+数据的有序性，与叶子节点又有链表相连，他的查询效率会比Hash快的多。

### 索引失效的常见场景

- 使用 `OR` 关键字会导致索引失效，不过如果要想使用OR 又不想让索引失效，那就得需要为`or`条件中的每个列都建立索引。这很显然是和上面的不要建立太多的索引相违背。
- 联合索引如果不遵循最左前缀原则，那么索引也将失效
- 使用模糊查询的时候以%开头也会导致索引失效

```mysql
select * from `user` where `name` like '%.txt';
```

- 对索引列进行运算，会导致索引失效

```mysql
select * from `user` where age - 1 = 10;
```

- 对索引列使用函数，会导致索引失效
- 索引列如果使用了隐式转换也会导致索引失效

#### 覆盖索引

覆盖索引（covering index ，或称为索引覆盖）只需要通过索引就可以返回查询所需要的数据，而不必通过二级索引查到主键之后再去查询聚簇索引中的数据，避免了回表的产生，显著提升性能。如，表covering_index_sample中有一个普通索引idx_key1_key2(key1,key2),当我们通过SQL语句：select key2 from covering_index_sample where key1 = 'keytest';的时候，就可以通过覆盖索引查询，无需回表。


覆盖索引是联合索引查询时的最优情况，不需要回表。

#### 视图、游标

视图是一种虚拟的表，通常是一个表或者多个表的行或列的子集，具有和物理表相同的功能。视图是虚拟的表，与包含数据的表不一样，视图只包含使用时动态检索数据的查询。

使用视图可以简化复杂的 sql 操作，隐藏具体的细节，保护数据；视图创建后，可以使用与表相同的方式利用它们。

```mysql
create view faculty as
select ID, name, dept_name from instructor;
```

游标是对查询出来的结果集作为一个单元来有效的处理。一般不使用游标，但是需要逐条处理数据的时候，游标显得十分重要。

#### 完整性约束

保证用户对数据库所做的修改不会破坏数据库的一致性，防止不符合规范的书籍进入数据库。

**实体完整性约束**

要求表中每一行数据不重复，包括主键约束（primary key不能为空）、唯一约束（数据唯一，可以为null）、check约束（指定属性值满足的条件）等

```mysql
create table section (
	course_id varchar(8),
	sec_id varchar(8),
	semester varchar(6),
	...
	check(semester in ('Fall', 'Winter', 'Spring', 'Summer')));
```

**参照完整性约束**

若属性（属性组）X 是关系 R 的外码，它对应另一个关系S的主键，那么 R 中每个元组在 X上的值要么取 NULL，要么等于S中主键的某个值。

```mysql
create table course(
	course_id varchar(8),
	dept_name varchar(20),
	...
	foreign key(dept_name) references department);
```

**用户自定义完整性约束**

用户可以定义不属于其它任何完整性分类的特定业务规则。所有的完整性类型都支持用户自定义完整性（CREATE TABLE 中的所有列级和表级约束、存储过程和触发器）。

#### 存储过程

存储过程是一组为了完成特定功能的SQL 语句集，事先经过编译并存储在数据库中，用户可通过指定存储过程的名字并给定参数(需要时)来调用执行。

好处：

使用存储过程的好处：

- 代码封装，保证了一定的安全性；
- 重复使用。存储过程可以重复使用，从而可以减少数据库开发人员的工作量。
- 由于是预先编译，因此具有很高的性能。

**创建存储过程**

语句：

> delimiter $
> create procedure 存储过程名称()
> begin
> SQL语句;
> end$

释义：delimiter的中文解释为‘分隔符’，表示将“$”代替“;”设置为分隔符,因为在begin后的SQL语句中需要以";"结尾,所以就要设置一个与";"区分开的分隔符。

包含 in、out 和 inout 三种参数。

变量赋值：set，select into 语句。

每次只能给一个变量赋值，不支持集合的操作。

```sql
delimiter //

create procedure myprocedure( out ret int )
    begin
        declare num int default 0;
        set num = num + 10;
        declare y int;
        select sum(col1)
        from mytable
        into y;
        select y*y into ret;
    end //

delimiter ;
```

```sql
call myprocedure(@ret);
select @ret;
```

https://segmentfault.com/a/1190000039248897

存储过程和函数的区别

- 一般来说，存储过程实现的功能要复杂一点，而函数的实现的功能针对性比较强。存储过程，功能强大，可以执行包括修改表等一系列数据库操作；用户定义函数不能用于执行一组修改全局数据库状态的操作。
- 函数只能返回一个变量；而存储过程可以返回多个。
- 存储过程一般是作为一个独立的部分来执行，而函数可以作为查询语句的一个部分来调用，由于函数可以返回一个表对象，因此它可以在查询语句中位于FROM关键字的后面。

https://www.cnblogs.com/youxin/p/3568379.html

#### 触发器

触发器对表进行插入、更新、删除等操作时会自动执行的特殊存储过程。

DDL触发器它们会影响多种数据定义语言语句而激发，这些语句有create、alter、drop语句。DML触发器分为：after触发器（之后触发，insert、update、delete触发器）和instead of 触发器 （之前触发）。

https://www.cnblogs.com/hoojo/archive/2011/07/20/2111316.html

#### SQL和MySQL

SQL是结构化查询语言；

MySQL是一个关系型数据库管理系统。可以向MySQL数据库提交SQL查询，以存储、检索、修改或删除数据。

#### Mysql的优化（高频，索引优化，性能优化）

高频访问：

* 分表分库：将数据库表进行水平拆分，减少表的长度
* 增加缓存： 在web和DB之间加上一层缓存层
* 增加数据库的索引：在合适的字段加上索引，解决高频访问的问题

并发优化：

* 主从读写分离：只在主服务器上写，从服务器上读
* 负载均衡集群：通过集群或者分布式的方式解决并发压力

#### 数据库高并发的解决方案

1. 在web服务框架中加入缓存。在服务器与数据库层之间加入缓存层，将高频访问的数据存入缓存中，减少数据库的读取负担。
2. 增加数据库索引。提高查询速度。（不过索引太多会导致速度变慢，并且数据库的写入会导致索引的更新，也会导致速度变慢）
3. 主从读写分离，让主服务器负责写，从服务器负责读。
4. 将数据库进行拆分，使得数据库的表尽可能小，提高查询的速度。
5. 使用分布式架构，分散计算压力。

#### MYSQL数据库引擎介绍，innodb和myisam的特点与区别

* InnoDB ： InnoDB是mysql的默认引擎，支持事务（默认隔离级别为可重复读），支持外键约束（增加了表之间的耦合度，但会降低表的查询速度），使用的锁粒度为行级锁，可以支持更高的并发。
* MyISAM ： 支持大文件，支持联合索引，查询速度比较快，但是不支持事务，使用的锁粒度为表级锁。
* MEMORY ： memory将表中的数据保存在内存里，支持哈希索引，适合数据比较小而且频繁访问的场景
* CSV
* blackhole

#### 数据库引擎InnoDB与MyISAM的区别

**InnoDB**

- 是 MySQL 默认的事务型存储引擎，只有在需要它不支持的特性时，才考虑使用其它存储引擎。
- 实现了四个标准的隔离级别，默认级别是可重复读(REPEATABLE READ)。在可重复读隔离级别下，通过多版本并发控制(MVCC)+ 间隙锁(Next-Key Locking)防止幻影读。
- 主索引是聚簇索引，在索引中保存了数据，从而避免直接读取磁盘，因此对查询性能有很大的提升。
- 内部做了很多优化，包括从磁盘读取数据时采用的可预测性读、能够加快读操作并且自动创建的自适应哈希索引、能够加速插入操作的插入缓冲区等。
- 支持真正的在线热备份。其它存储引擎不支持在线热备份，要获取一致性视图需要停止对所有表的写入，而在读写混合场景中，停止写入可能也意味着停止读取。

**MyISAM**

- 设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。
- 提供了大量的特性，包括压缩表、空间数据索引等。
- 不支持事务。
- 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入(CONCURRENT INSERT)。

**总结**

1）事务：MyISAM不支持，InnoDB支持 

2）锁级别：MyISAM 表级锁，InnoDB 行级锁及外键约束 

3）MyISAM存储表的总行数；InnoDB不存储总行数；

4）MyISAM采用非聚集索引，B+树叶子存储指向数据文件的指针。InnoDB主键索引采用聚集索引，B+树叶子存储数据

**适用场景**：

MyISAM适合：插入不频繁，查询非常频繁，如果执行大量的SELECT，MyISAM是更好的选择， 没有事务。

InnoDB适合：可靠性要求比较高，或者要求事务；表更新和查询都相当的频繁， 大量的INSERT或UPDATE

#### 数据库连接池的作用

原理：在内部对象池中，维护一定数量的数据库连接，并对外暴露数据库连接的获取和返回方法.

作用：

1. **资源重用**，避免了频繁创建、释放连接引起的大量性能开销。在减少系统消耗的基础上，增进了系统环境的平稳性（减少内存碎片以级数据库临时进程、线程的数量）
2. **更快的系统相应速度**，数据库连接池在初始化过程中，往往已经创建了若干数据库连接置于池内备用。此时连接池的初始化操作均已完成。对于业务请求处理而言，直接利用现有可用连接，避免了数据库连接初始化和释放过程的时间开销，从而缩减了系统整体响应时间。
3. **新的资源分配手段**， 对于多应用共享同一数据库的系统而言，可在应用层通过数据库连接的配置，实现数据库连接技术。
4. **统一的连接管理，避免数据库连接泄露**，在较为完备的数据库连接池实现中，可根据预先的连接占用超时设定，强制收回被占用的连接，从而避免了常规数据库连接操作中可能出现的资源泄露

#### Mysql的表空间方式，各自特点

* **共享表空间**：指的是数据库的所有的表数据，索引文件全部放在一个文件中，默认这个共享表空间的文件路径在 data 目录下。 
* **独立表空间**：每一个表都将会生成以独立的文件方式来进行存储。 优点：当表被删除时这部分空间可以被回收；可以更快的恢复和备份单个表；将单个表复制到另一个实例会很方便； 缺点：mysqld会维持很多文件句柄，表太多会影响性能。如果很多表都增长会导致碎片问题

#### 数据库表锁和行锁

###### 表锁

不会出现死锁，发生锁冲突几率高，并发低。

MyISAM在执行查询语句（select）前，会自动给涉及的所有表加读锁，在执行增删改操作前，会自动给涉及的表加写锁。

MySQL的表级锁有两种模式：表共享读锁和表独占写锁。

读锁会阻塞写，写锁会阻塞读和写

- 对MyISAM表的读操作，不会阻塞其它进程对同一表的读请求，但会阻塞对同一表的写请求。只有当读锁释放后，才会执行其它进程的写操作。
- 对MyISAM表的写操作，会阻塞其它进程对同一表的读和写操作，只有当写锁释放后，才会执行其它进程的读写操作。

MyISAM不适合做写为主表的引擎，因为写锁后，其它线程不能做任何操作，大量的更新会使查询很难得到锁，从而造成永远阻塞。

###### 行锁

会出现死锁，发生锁冲突几率低，并发高。

在MySQL的InnoDB引擎支持行锁，与Oracle不同，MySQL的行锁是通过索引加载的，也就是说，行锁是加在索引响应的行上的，要是对应的SQL语句没有走索引，则会全表扫描，行锁则无法实现，取而代之的是表锁，此时其它事务无法对当前表进行更新或插入操作。

**行锁的实现需要注意：**

- 行锁必须有索引才能实现，否则会自动锁全表，那么就不是行锁了。
- 两个事务不能锁同一个索引。
- insert，delete，update在事务中都会自动默认加上排它锁。

#### 间隙锁

间隙锁（Gap Lock）是对索引记录之间的间隙的锁，或者是对第一个索引记录之前或最后一个索引记录之后的间隙的锁，用于防止幻读。

Next-Key Lock是行锁和间隙锁的组合，当InnoDB扫描索引记录的时候，会首先对索引记录加上行锁（Record Lock），再对索引记录两边的间隙加上间隙锁。加上间隙锁之后，其他事务就不能在这个间隙修改或者插入记录。

 Innodb自动使用间隙锁的条件：
（1）必须在Repeatable Read级别下
（2）检索条件必须有索引（没有索引的话，mysql会全表扫描，那样会锁定整张表所有的记录，包括不存在的记录，此时其他事务不能修改不能删除不能添加）

https://www.cnblogs.com/aspirant/p/9177978.html

#### 数据库为什么要进行分库和分表

分库与分表的目的在于，减小数据库的单库单表负担，提高查询性能，缩短查询时间。

垂直分表：将一个表按字段分成多表，每个表存储其中一部分字段。

水平分表：在同一个数据库内，把同一个表的数据按一定规则拆到多个表中（对数据的拆分，不影响表结构）。

垂直分库：垂直分库是指按照业务将表进行分类，分布到不同的数据库上面，每个库可以放在不同的服务器上，它的核心理念是专库专用。 

水平分库：水平分库就是把同一个表的数据按一定规则拆到不同的数据库中，每个库可以放在不同的服务器上。

#### 数据库事务

数据库事务通常包含了一个序列的对数据库的读/写操作，包含有以下两个目的：

- 为数据库操作序列提供了一个从失败中恢复到正常状态的方法，同时提供了数据库即使在异常状态下仍能保持一致性的方法。
- 当多个应用程序在并发访问数据库时，可以在这些应用程序之间提供一个隔离方法，以防止彼此的操作互相干扰。

事务是构成单一逻辑工作单元的操作集合，实现事务要能够满足ACID特性，而这些主要是靠**日志恢复**和**并发控制**实现的。

* **日志恢复**：数据库里有两个日志，一个是redo log，一个是undo log。redo log记录的是已经成功提交的事务操作信息，用来恢复数据，保证事务的**持久性**。undo log记录的是事务修改之前的数据信息，用来回滚数据，保证事务的**原子性**。
* **并发控制**：并发控制主要靠**读写锁**和**MVCC（多版本并发控制**）来实现。读写锁包括共享锁和排他锁，保证事务的**隔离性**。MVCC通过为数据添加时间戳来实现。

**ACID**

- 原子性（Atomicity）：即不可分割性，事务中的操作要么全不做，要么全做
- 一致性（Consistency）：事务的执行结果必须使数据库从一个一致性状态到另一个一致性状态，满足[完整性约束](https://baike.baidu.com/item/数据完整性约束)
- 隔离性（Isolation）：事务不会受另一个并发执行的事务的影响
- 持久性（Durability）：事务一旦提交,其对数据库的更新就是持久的，即便系统故障也不会丢失

#### 常见并发异常

- 脏写：事务回滚了其他事务对数据项的已提交修改。如事务1对数据A回滚,导致事务2对A的已提交修改也被回滚了。

<img src="https://img2018.cnblogs.com/blog/1422237/201811/1422237-20181122103114390-240225061.png" alt="img" style="zoom: 33%;" />

- 脏读：一个事务读取了另一个事务未提交的数据。如在事务1对A的处理过程中,事务2读取了A的值,但之后事务1回滚,导致事务2读取的A是未提交的脏数据。

<img src="https://img2018.cnblogs.com/blog/1422237/201811/1422237-20181122103137124-343962409.png" alt="img" style="zoom:33%;" />

- 不可重复读：一个事务对同一数据的读取结果前后不一致。脏读和不可重复读的区别在于:前者读取的是事务未提交的脏数据,后者读取的是事务已经提交的数据,只不过因为数据被其他事务修改过导致前后两次读取的结果不一样。如由于事务2对A的已提交修改,事务1前后两次读取的结果不一致。

<img src="https://img2018.cnblogs.com/blog/1422237/201811/1422237-20181122103147288-996731595.png" alt="img" style="zoom:33%;" />

- 幻读：事务读取某个范围的数据时，因为其他事务的操作导致前后两次读取的结果不一致。幻读和不可重复读的区别在于,不可重复读是针对确定的某一行数据而言,而幻读是针对不确定的多行数据。因而幻读通常出现在带有查询条件的范围查询中，如事务1查询A<5的数据,由于事务2插入了一条A=4的数据,导致事务1两次查询得到的结果不一样。

<img src="https://img2018.cnblogs.com/blog/1422237/201811/1422237-20181122103158043-533492513.png" alt="img" style="zoom:33%;" />

#### 数据库的隔离级别

所有事务隔离级别都不允许出现脏写。

* 读未提交 Read Uncommitted: 最低级别的隔离，不能解决以上问题
* 读已提交 Read committed:   可以避免脏读的发生 
* 可重复读 Reapeatable read:  确保事务可以多次从一个字段中读取相同的值，在该事务执行期间，禁止其他事务对此字段的更新，可以避免脏读和不可重复读。 通过锁行来实现 
* 串行化 Serializable  最严格的事务隔离机制，要求所有事务被串行执行，可以避免以上所有问题。 通过锁表来实现

Oracle的默认隔离级别是**读已提交**，实现了四种隔离级别中的读已提交和串行化隔离级别

MySQL的默认隔离级别是**可重复读**，并且实现了所有四种隔离级别

#### 数据的锁的种类，加锁的方式

以MYSQL为例，

* 按照类型来分有乐观锁和悲观锁
* 根据粒度来分有行级锁，页级锁（一次锁定相邻的一组记录），表级锁（粒度一个比一个大） （仅BDB，Berkeley Database支持页级锁）
* 根据作用来分有共享锁（读锁）和排他锁（写锁）。

#### 乐观锁与悲观锁解释一下

一般的数据库都会支持并发操作，在并发操作中为了避免数据冲突，所以需要对数据上锁，乐观锁和悲观锁就是两种不同的上锁方式。

悲观锁假设数据在并发操作中一定会发生冲突，所以在数据开始读取的时候就把数据锁住。而乐观锁则假设数据一般情况下不会发生冲突，所以在数据提交更新的时候，才会检测数据是否有冲突。

#### 乐观锁与悲观锁是怎么实现的

悲观锁利用数据库本身提供的锁机制来实现。

悲观锁要求在整个过程中一直与数据库有一条连接，因为上一个事务完成后才能让下一个事务执行，这个过程是串行的。

乐观锁有三种常用的实现形式：

* 一种是在执行事务时把整个数据都拷贝到应用中，在数据更新提交的时候比较数据库中的数据与新数据，如果两个数据一摸一样则表示没有冲突可以直接提交，如果有冲突就要交给业务逻辑去解决。
* 一种是使用版本戳来对数据进行标记，数据每发生一次修改，版本号就增加1。某条数据在提交更新时，如果数据库中的版本号与自己的一致，就说明可以更新，否则就认为是过期数据需要处理。
* 最后一种采用时间戳对数据最后修改的时间进行标记。与上一种类似。

#### mysql的MVCC

MVCC，多版本并发控制，为每个数据项维护多个版本，使得数据库读不需要加锁，提高数据库的并发处理能力。

每行记录有两个隐藏列，记录了行的事务ID（DB_TRX_ID）和指向上一次修改的回滚指针（DB_ROLL_PTR）。

InnoDB中的每个事务都有一个唯一的事务ID，在事务开始时向InnoDB申请，按时间先后严格递增。

MVCC在mysql中的实现依赖于undo log和read view。

undo log保存未提交数据之前的数据版本，分为Insert undo log和update undo log。insert undo log在事务进行插入操作时产生，在事务回滚时需要，事务提交后可以被立即丢弃，update undo log在事务进行update或delete操作时参数，用于回滚事务和快照读，所以在事务提交后不能被立即丢弃，只有在快照读不涉及该log时，对应的log在会被purge线程统一清除。

read view记录当前系统中活跃事务的ID列表m_ids，min_trx_id表示事务列表中最小的事务ID，max_trx_id表示事务列表中最大的事务ID，版本链数据对当前事务可见通过以下规则判断：

- 如果数据版本trx_id < min_trx_id，表示生成该版本的事务在生成read view前已经提交，该版本可见
- 如果数据版本trx_id > max_trx_id，表示生成该版本的事务在生成read view后才生成，该版本不可见
- 如果min_trx_id <= trx_id <= max_trx_id，当trx_id在m_ids中，表示在创建read view时生成该版本的事务还是活跃的，该版本不可见，如果trx_id不在m_ids中，说明创建read view时生成该版本的事务已经提交，该版本可见。

在读已提交下，每次查询都会生成read view，所以可以读到已经提交的数据；在可重复读隔离级别下，事务在开始第一次查询时生成read view，以后的select都使用第一次生成的read view，解决了不可重复读问题。

#### 并发控制

并发控制技术是实现事务隔离性以及不同隔离级别的关键。

- 乐观并发控制:对于并发执行可能冲突的操作,假定其不会真的冲突,允许并发执行,直到真正发生冲突时才去解决冲突,比如让事务回滚。
- 悲观并发控制:对于并发执行可能冲突的操作,假定其必定发生冲突,通过让事务等待(锁)或者中止(时间戳排序)的方式使并行的操作串行执行。

**基于锁的并发控制**

对于并发可能冲突的操作,比如读-写,写-读,写-写,通过锁使它们由并行变为串行执行。

- 共享锁(S): 共享锁是读操作的时候创建的锁，一个事务对数据加上共享锁之后，其他事务只能对数据再加共享锁，不能进行写操作直到释放所有共享锁。
- 排它锁(X): 排他锁是写操作时创建的锁，事务对数据加上排他锁之后其他任何事务都不能对数据加任何的锁（即其他事务不能再访问该数据）

可能出现的问题:

- 死锁:多个事务持有锁并互相循环等待其他事务的锁导致所有事务都无法继续执行。

- 饥饿:数据项A一直被加共享锁,导致事务一直无法获取A的排他锁。

**两阶段锁**

两阶段锁协议（2PL）是一种能够保证事务可串行化的协议，它将事务的获取锁和释放锁划分成了增长（Growing）和缩减（Shrinking）两个不同的阶段。

在增长阶段，一个事务可以获得锁但是不能释放锁；而在缩减阶段事务只可以释放锁，并不能获得新的锁。

**基于时间戳的并发控制**

对于并发可能冲突的操作,基于时间戳排序规则选定某事务继续执行,其他事务回滚。

系统会在每个事务开始时赋予其一个唯一的时间戳TS(T)，这个时间戳可以是系统时钟也可以是一个不断累加的计数器值，当事务回滚时会为其赋予一个新的时间戳，先开始的事务时间戳小于后开始事务的时间戳。

每一个数据项Q有两个时间戳相关的字段：

- W-TS(Q)：成功执行write(Q)的所有事务的最大时间戳

- R-TS(Q)：成功执行read(Q)的所有事务的最大时间戳

时间戳排序规则如下:

1. 假设事务T发出read(Q),T的时间戳为TS
   - 若TS(T)<W-TS(Q)：则T需要读入的Q已被覆盖，此read操作将被拒绝，T回滚。
   - 若TS(T)>=W-TS(Q)：则执行read操作，同时把R-TS(Q)设置为TS(T)与R-TS(Q)中的最大值
2. 假设事务T发出write(Q)
   - 若TS(T)<R-TS(Q)：write操作被拒绝，T回滚。
   - 若TS(T)<W-TS(Q)：则write操作被拒绝，T回滚。
   - 其他情况：系统执行write操作，将W-TS(Q)设置为TS(T)。

基于时间戳排序和基于锁实现的本质一样：对于可能冲突的并发操作，以串行的方式取代并发执行，因而它也是一种悲观并发控制。它们的区别主要有两点:

- 基于锁是让冲突的事务进行等待，而基于时间戳排序是让冲突的事务回滚。
- 基于锁冲突事务的执行次序是根据它们申请锁的顺序，先申请的先执行；而基于时间戳排序是根据特定的时间戳排序规则。

**基于有效性检查的并发控制**

事务对数据的更新首先在自己的私有空间进行,等到要写回数据库时才进行有效性检查,对不符合要求的事务进行回滚。

- Read阶段：从数据库中读取数据并在私有空间完成写操作，这个时候其实并没有实际写入数据库。维护当前事务的读写集合，RS、WS。
- Validate阶段：根据某种可串行化标准检查待提交事务是否满足可串行化调度。
- Write（只读事务不需要）：若Validate成功，就将事务私有空间中的数据更新写入数据库使其全局可见。

有效性检查，记录每个事务的开始时间START(T)，验证时间VAL(T)，完成写入时间FIN(T)

**基于快照隔离的并发控制**

快照隔离是多版本并发控制(mvcc)的一种实现方式。

数据库为每个数据项维护多个版本(快照)，每个事务只对属于自己的私有快照进行更新，在事务真正提交前进行有效性检查，使得事务正常提交更新或者失败回滚。

由于快照隔离导致事务看不到其他事务对数据项的更新，为了避免出现丢失更新问题，可以采用以下两种方案避免：

- 先提交者获胜:对于执行该检查的事务T，判断是否有其他事务已经将更新写入数据库,是则T回滚否则T正常提交。
- 先更新者获胜:通过锁机制保证第一个获得锁的事务提交其更新,之后试图更新的事务中止。

事务间可能冲突的操作通过数据项的不同版本的快照相互隔离，到真正要写入数据库时才进行冲突检测。因而这也是一种乐观并发控制。

#### 范式

范式（Paradigm）是符合某一种级别的关系模式的集合。关系数据库中的关系必须满足一定的要求，满足不同程度要求的为不同范式。目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、Boyce-Codd范式（BCNF）、第四范（4NF）和第五范式（5NF）。

满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多要求的称为第二范式（2NF），其余范式以次类推。一般说来，数据库只需满足第三范式（3NF）就行了。

**第一范式**

列不可分（消除可分数据项）

第一范式（1NF）是指数据库表的每一列都是不可分割的基本数据项，同一列中不能有多个值，即实体中的某个属性不能有多个值或者不能有重复的属性。

说明：在任何一个关系数据库中，第一范式（1NF）是对关系模式的设计基本要求，一般设计中都必须满足第一范式（1NF）。

**第二范式**

主键依赖（消除部分依赖）

第二范式（2NF）要求实体的属性完全依赖于主关键字。所谓完全依赖是指不能存在仅依赖主关键字一部分的属性。如果存在，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与原实体之间是一对多的关系。为实现区分通常需要为表加上一个列，以存储各个实例的唯一标识。

**第三范式**

表不可分（消除传递依赖）

在1NF基础上，任何非主属性不依赖于其它非主属性[在2NF基础上消除传递依赖]。

首先是 2NF，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。 

要求一个数据库表中不包含已在其它表中已包含的非主关键字信息。

<img src="E:\我的坚果云\Notes\Work\笔记\图片\955092-20200901092633851-1897302181.png" style="zoom:67%;" />

<img src="E:\我的坚果云\Notes\Work\笔记\图片\955092-20200901092645697-1510000657.png" style="zoom:67%;" />

**BCNF范式（确保主键之间没有传递依赖）**
主键有可能是由多个属性组合成的复合主键，那么多个主键之间不能有传递依赖。也就是复合主键之间谁也不能决定谁，相互之间没有关系。

#### 完整性约束包括哪些？

数据完整性(Data Integrity)是指数据的精确(Accuracy)和可靠性(Reliability)。

分为以下四类：

1. 实体完整性：规定表的每一行在表中是惟一的实体。
2. 域完整性：是指表中的列必须满足某种特定的数据类型约束，其中约束又包括取值范围、精度等规定。
3. 参照完整性：是指两个表的主关键字和外关键字的数据应一致，保证了表之间的数据的一致性，防止了数据丢失或无意义的数据在数据库中扩散。
4. 用户定义的完整性：不同的关系数据库系统根据其应用环境的不同，往往还需要一些特殊的约束条件。用户定义的完整性即是针对某个特定关系数据库的约束条件，它反映某一具体应用必须满足的语义要求。

与表有关的约束：包括列约束(NOT NULL（非空约束）)和表约束(PRIMARY KEY、foreign key、check、UNIQUE) 。

#### MySQL 支持事务吗？

在缺省模式下，MySQL 是 autocommit 模式的，所有的数据库更新操作都会即时提交，所以在缺省情况下，MySQL 是不支持事务的。

但是如果你的 MySQL 表类型是使用 InnoDB Tables 或 BDB tables 的话，你的MySQL 就可以使用事务处理,使用 SETAUTOCOMMIT=0 就可以使 MySQL 允许在非 autocommit 模式，在非autocommit 模式下，你必须使用 COMMIT 来提交你的更改，或者用 ROLLBACK来回滚你的更改。

MYSQL的事务处理主要有**两种**方法

　　1.用begin,rollback,commit来实现
　　　 begin开始一个事务
　   　  rollback事务回滚
  　　   commit 事务确认
　　2.直接用set来改变mysql的自动提交模式
  　　   mysql默认是自动提交的，也就是你提交一个query，就直接执行！可以通过
  　　   set autocommit = 0 禁止自动提交
  　　   set autocommit = 1 开启自动提交
    来实现事务的处理

#### CAP

CAP 定理是分布式系统设计中最基础，也是最为关键的理论。它指出，分布式数据存储不可能同时满足以下三个条件。

- **一致性（Consistency）**：每次读取要么获得最近写入的数据，要么获得一个错误。
- **可用性（Availability）**：每次请求都能获得一个（非错误）响应，但不保证返回的是最新写入的数据。
- **分区容错（Partition tolerance）**：尽管任意数量的消息被节点间的网络丢失（或延迟），系统仍继续运行。

在分布式环境中，每个节点都不是可靠的，各节点之间的通信也可能出问题。当某些节点出现故障（或者节点本身的故障，或者部分网络故障）时，整个系统就产生了所谓的”分区“。当系统产生”分区“的时候，如果还能对外提供比较好的服务（例如较好的一致性和可用性），就可以说该系统具有较好的”分区容错性“（Partition Tolerance）。

由于在分布式系统中，P 是必须要保留的，所以要在 C 和 A 间进行取舍。假如要保证服务的可用性，就选择 AP 模型，而要保证一致性的话，就选择 CP 模型。

**CAP理论在现实中的应用**

既然理论是这样，我们就不要浪费时间去设计完美的分布式系统，这就是方法论起的作用。

考虑到在分布式场景中，系统产生分区的情况无法避免，我们就只能尽量提供一个比较好的”分区容错性“的产品。换句话说，我们需要在系统出现分区的时候，在一致性和可用性之间做权衡。

- CP：优先保证数据的一致性，在数据没有一致的情况下，可以适当降低系统的可用性，比如放弃当前的请求，让客户端重试；或者降低对客户端的响应速度（比如银行转账结束时等待5s的提示界面）。ZooKeeper被设计为在分布式系统中协调服务、保证各服务节点数据一致的产品，就是CP的例子。还有各种分布式数据库产品，如Redis、HBase，也都是偏向数据一致性的CP的例子。
- AP：优先保证系统的可用性，降低数据的一致性诉求。比如电商网站的下单界面展示的可购买数量，这个是时时变化的。如果要保证一致性，则需要系统时时刷新获取最新的数据，势必会影响网站的响应速度，也就降低了可用性，影响用户体验（可以改为在真正下单的那一刻再提示库存是否满足下单条件）。

如果有网络分区产生，系统就必须在 A 和 C 之间取得平衡，否则当系统运行在无网络分区情况下，系统需要在 L（延迟，即节点同步数据到到其它节点的时间）和 C 之间取得平衡。

https://segmentfault.com/a/1190000038918928

#### BASE理论

BASE理论是对CAP理论的延伸，作用是保证系统的可用性，核心思想是即使无法做到强一致性（StrongConsistency，CAP的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性（Eventual Consitency）。

##### （Basically Available）基本可用

在分布式系统出现故障的时候，允许损失部分可用性，即保证核心可用。

##### （Soft State）软状态

接受一段时间的状态不同步，及中间状态，而该中间状态不影响系统整体可用性。这里的中间状态就是CAP理论中的数据不一致性。即允许系统在多个不同节点的数据副本存在数据延时。这个时间期限取决于网络延时、系统负载、数据复制方案设计等等因素。

##### （Eventually Consistent）最终一致性

上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后系统能够保证在没有其他新的更新操作的情况下，所有副本保持数据一致性，从而达到数据的最终一致性。因此所有客户端对系统的数据访问最终都能够获取到最新的值。

https://juejin.cn/post/6844903621495095304

#### 逻辑时钟

**逻辑时钟指的是分布式系统中用于区分事件发生顺序的时间机制。**逻辑时钟是为了解决分布式系统中的时序问题，即如何定义a在b之前发生。

分布式系统需交互的三类事件，发生于

- 节点内：同一节点内的事件存在数据传播
- 发送事件：一个节点向另一节点发送数据
- 接受事件：一个节点接受另一节点发送来的数据

首先需要定义先后关系(happened before)，我把事件 a 发生在 b 之前定义为 a → b。以下三种条件都满足 a → b:

1. a和b是同一个进程内的事件，a发生在b之前，则 a → b。
2. a和b在不同的进程中，a是发送进程内的发送事件，b是同一消息接收进程内的接收事件，则 a → b。
3. 如果a → b并且b → c，则a → c。

如果a和b没有先后关系，则称两个事件是并发的，记作 a || b。

**逻辑时钟算法**

分布式系统中每个进程Pi保存一个本地逻辑时钟值Ci，由单调递增的软件计数器维护，Ci (a) 表示进程Pi发生事件a时的逻辑时钟值，Ci的更新算法如下：

1. 进程Pi每发生一次事件，Ci加1。
2. 进程Pi给进程Pj发送消息，Ci加1并在消息中带上自己的本地逻辑时钟Ci。
3. 进程Pj接收消息，更新Cj为 max (Ci, Cj) + 1

可以得出，**对于任意两个事件a和b，如果 a → b，那么 C (a) < C (b)。但如果 C (a) < C (b)，不能得出 a → b。**

#### 向量时钟

向量时钟算法是在 Lamport 逻辑时钟的基础上进行了改良，用于在分布式系统中描述事件因果关系的算法。

假设分布式系统中有 N 个进程，每个进程都有一个本地的向量时间戳 Ti，向量时钟算法实现如下：

1. 对于进程 i 来说，Ti[i] 是进程 i 本地的逻辑时间
2. 当进程 i 当有新的事件发生时，Ti[i] = Ti[i] + 1
3. 当进程 i 发送消息时将它的向量时间戳(MT=Ti)附带在消息中。
4. 接受消息的进程 j 更新本地的向量时间戳：Tj[k] = max(Tj[k], MT[k]) for k = 1 to N。（MT即消息中附带的向量时间戳）

<img src="C:\Users\rbai\AppData\Roaming\Typora\typora-user-images\image-20220309152028945.png" alt="image-20220309152028945" style="zoom: 33%;" />

向量时钟可以用来检测分布式系统中多副本更新的数据冲突问题。

https://juejin.cn/post/6844904147053969421

#### 一致性模型

**严格一致性模型**

- 任何一次读都能读取到某个数据最近的一次写的数据
- 所有进程看到的操作顺序都跟全局时钟下的顺序一致

**顺序一致性模型**

- 任何一次读写操作都按照某种特定的顺序
- 所有进程看到的读写操作顺序都保持一致

放弃了严格一致性中全局时钟的约束，改为分布式逻辑时钟，保证所有进程对数据的读写顺序保持一致，未保证与实际顺序一致。

<img src="https://img-blog.csdn.net/20180721214208274?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoYW8yMDE2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述" style="zoom:80%;" />

**因果一致性模型**

- 所有进程必须以相同的顺序看到具有因果关系的读写操作
- 不同进程可以以不同的顺序看到并发的读写操作

**FIFO一致性模型**

- 所有进程以某种单一进程提出写操作的顺序看到这些写操作
- 但是，不同进程可以以不同的顺序看到不同的进程提出的写操作

在一个进程内，所有的写操作在对外可以被看到的时候，必须是一致的，但是两个进程的写操作的顺序被看到的时候，可以不同，就算是有因果关系也不保证。

**最终一致性模型**

不保证在任意时刻任意节点上的同一份数据都是相同的，但是随着时间的迁移，不同节点上的同一份数据总是在向趋同的方向变化。保证所有副本的数据最终在某个时刻会保持一致。

**以客户端为中心的一致性为单一客户端提供一致性保证，保证该客户端对数据存储的访问的一致性，但是它不为不同客户端的并发访问提供任何一致性保证。**

以客户端为中心的一致性包含了四种子模型：

- **单调读一致性**（Monotonic-read Consistency）：如果一个进程读取数据项 x 的值，那么该进程对于 x 后续的所有读操作要么读取到第一次读取的值要么读取到更新的值。即保证客户端不会读取到旧值。

  例子：实际业务中，某一用户读到了一个审批流程，在页面看到已经进行到了第四步，刷新了一下，防止可能路由到了另外一个数据副本，又回到了第三步，产生了比较莫名其妙的错误

- **单调写一致性**（Monotonic-write Consistency）：一个进程对数据项 x 的写操作必须在该进程对 x 执行任何后续写操作之前完成。即保证客户端的写操作是串行的，同一进程的写操作顺序执行。

  例子：在实际业务中，有一款游戏，用户打怪升级，杀死一个怪物增加100经验值，此时如果不满足单调写一致性，有可能导致用户杀死怪物后，经验值没有增加的问题

- **读写一致性**（Read-your-writes Consistency）：一个进程对数据项 x 执行一次写操作的结果总是会被该进程对 x 执行的后续读操作看见。即保证客户端能读到自己最新写入的值。

  例子：用户发了一个微博，如果后端mysql采用master-slave结构做读写分离，当并发量比较大时产生了延迟，结果他发完后没有在自己的微博列表看到刚刚发过的微博，这时产生了不好的用户体验。通常我们会规定在写后t时间内读取master的数据，t大于mysql的主从延迟时间

- **写读一致性**（Writes-follow-reads Consistency）：同一个进程对数据项 x 执行的读操作之后的写操作，保证发生在与 x 读取值相同或比之更新的值上。即保证客户端对一个数据项的写操作是基于该客户端最新读取的值。

  例子: 实际业务中，如果一个用户A在聊天室发了一条消息，用户B看到了，然后进行回复，如果副本间没有及时同步，下次请求路由到了另外一个副本，结果发现用户B发的回复还在，用户A发的内容没有了。出现了不好的用户体验。这种情况还不如用户B更晚看到用户A的消息

https://segmentfault.com/a/1190000018486647

#### 两阶段提交（2PC）

两阶段提交协议（Two-phase Commit，即2PC）是常用的分布式事务解决方案，它可以保证在分布式事务中，要么所有操作全部成功，要么全部失败，即实现 ACID 的原子性。

- 第一阶段，投票阶段：协调者向所有的参与者发送事务执行请求；参与者收到请求之后，执行事务但不提交，并记录事务日志；参与者将自己事务执行情况反馈给协调者。
- 第二阶段，提交阶段：如果所有的参与者都回复能够正常执行事务，协调者就向所有参与者发送 commit 通知，请求提交事务，参与者收到事务提交通知之后执行 commit 操作，然后释放占有的资源。如果任何一个参与者回复事务执行失败，协调者就向所有参与者发送事务 rollback 通知，请求回滚事务，参与者收到事务回滚通知之后执行 rollback 操作，然后释放占有的资源。

缺点：

- 单点故障：由于协调者的重要性，一旦协调者发生故障，就会影响整个数据库集群的运行，尤其在第二阶段，协调者发生故障，所有参与者会一直阻塞，无法继续完成事务操作。
- 同步阻塞：两阶段提交执行过程中，所有的参与者都需要听从协调者的统一调度，期间处于阻塞状态而不能从事其他操作，这样效率极其低下。
- 数据不一致：在第二阶段中，假设协调者发出了事务 commit 通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。

#### 三阶段提交（3PC）

三阶段提交协议（Three-Phase Commit， 3PC）引入超时机制，并在第一阶段和第二阶段中间插入了一个准备阶段，保证了在最后提交阶段前各节点的状态一致。

- CanCommit：协调者向各个参与者发送事务询问通知，询问是否可以执行事务操作，并等待响应。参与者收到 CanCommit 请求后，正常情况下，如果自身认为可以顺利执行事务，那么会反馈 确定信息，并进入预备状态，否则反馈否定信息。
- PreCommit：如果所有的参与者都返回确定信息，协调者向所有参与者发送PreCommit请求，参与者收到通知后执行事务但不提交，参与者将事务执行情况返回给协调者。如果任何一个参与者向 Coordinator 反馈了否定信息，或者在等待超时后，Coordinator 无法接收到所有参与者的反馈，那么就会中断事务，协调者向所有事务参与者发送 abort 请求，参与者无论是收到abort请求，还是等待超时，都会中断事务。
- DoCommit：如果所有的参与者都能正常执行事务，协调者向所有参与者发送 doCommit 请求，所有参与者在收到请求之后执行 commit 操作，释放占有的资源，并向协调者反馈事务提交结果。如何任何一个参与者事务执行失败，或者等待超时，协调者中断事务，向所有参与者发送abort请求，参与者收到abort请求后，回滚事务，释放占有的资源，并向协调者反馈事务回滚结果。如果参与者收不到来自协调者的doCommit或abort请求，会在等待超时后继续commit，相对于两阶段提交虽然降低了同步阻塞，但仍然无法完全避免数据的不一致。

#### Paxos算法

Paxos算法是基于消息传递且具有高度容错特性的一致性算法。

Paxos算法分为**两个阶段**。具体如下：

- **阶段一, Prepare：**

  (a) Proposer选择一个**提案编号N**，然后向**半数以上**的Acceptor发送编号为N的**Prepare请求**。

  (b) 如果一个Acceptor收到一个编号为N的Prepare请求，且N**大于**该Acceptor已经**响应过的**所有**Prepare请求**的编号，那么它就会将它已经**接受过的编号最大的提案（如果有的话）**作为响应反馈给Proposer，同时该Acceptor承诺**不再接受**任何**编号小于N的提案**。

- **阶段二, Accept：**

  (a) 如果Proposer收到**半数以上**Acceptor对其发出的编号为N的Prepare请求的**响应**，那么它就会发送一个针对**[N,V]提案**的**Accept请求**给**半数以上**的Acceptor。注意：V就是收到的**响应**中**编号最大的提案的value**，如果响应中**不包含任何提案**，那么V就由Proposer**自己决定**。

  (b) 如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor**没有**对编号**大于N**的**Prepare请求**做出过**响应**，它就**接受该提案**。

<img src="https://upload-images.jianshu.io/upload_images/1752522-44c5a422f917bfc5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Paxos算法流程" style="zoom:67%;" />

Learner 学习（获取）被选定的 value 有如下三种方案:

<img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/6/17/1640df96d5ab075d~tplv-t2oaga2asx-zoom-in-crop-mark:1304:0:0:0.awebp" alt="img" style="zoom:67%;" />

如何保证Paxos算法的活性

<img src="https://p1-jj.byteimg.com/tos-cn-i-t2oaga2asx/gold-user-assets/2018/6/17/1640df96ee547957~tplv-t2oaga2asx-zoom-in-crop-mark:1304:0:0:0.awebp" alt="img" style="zoom:67%;" />

https://juejin.cn/post/6844903621499289613

#### Raft算法

在分布式系统场景下，一致性指的是多个副本对外呈现的状态。如前面提到的顺序一致性、线性一致性，描述了多节点对数据状态的共同维护能力。而共识，则特指在分布式系统中多个节点之间对某个事情（例如多个事务请求先执行谁，某个键对应的值）达成一致看法的过程。因此，达成某种共识并不意味着就保障了一致性。

共识算法就是保证一个集群的多台机器协同工作，在遇到请求时，数据能够保持一致。即使遇到机器宕机，整个系统仍然能够对外保持服务的可用性。

**Leader Election 领导选举**

集群中每个节点只能处于Leader、Follower和Candidate三种状态的一种：

Follower从节点：

- 节点默认是follower；
- 如果刚刚开始或和leader通信超时，follower会发起选举，增加节点本地的current term, 变成candidate，然后去竞选leader；
- 如果收到其他candidate的竞选投票请求，按照先来先得 & 每个任期只能投票一次 & candidate存储的日志至少要和follower一样新（安全性准则）的投票原则投票。

Candidate候选者：

- follower发起选举后就变为candidate，会向其他节点拉选票。candidate的票会投给自己，所以不会向其他节点投票；
- 如果获得超过半数的投票，candidate变成leader，然后马上和其他节点通信，表明自己的leader的地位；
- 如果选举超时，重新发起选举；
- 如果遇到更高任期term的leader的通信请求，转化为follower。

Leader主节点：

- 成为leader节点后，此时可以接受客户端的数据请求，负责日志同步；
- 如果遇到更高任期term的candidate的通信请求，这说明candidate正在竞选leader，此时之前任期的leader转化为follower，且完成投票；
- 如果遇到更高任期term的leader的通信请求，这说明已经选举成功新的leader，此时之前任期的leader转化为follower。

Raft算法把时间轴划分为不同任期Term。每个任期Term都有自己的编号TermId，该编号全局唯一且单调递增。如下图，**每个任务的开始都 Leader Election 领导选举**。如果选举成功，则进入维持任务Term阶段，此时leader负责接收客户端请求并，负责复制日志。Leader和所有follower都保持通信，如果follower发现通信超时，TermId递增并发起新的选举。如果选举成功，则进入新的任期。如果选举失败，TermId递增，然后重新发起选举直到成功。

<img src="http://dockone.io/uploads/article/20211031/de41e74f45d792e5232ca9b5cdda3487.jpg" alt="3.jpg" style="zoom:67%;" />

**Log Replication 日志复制**

选举Leader成功后，整个集群就可以正常对外提供服务了。Leader接收所有客户端请求，然后转化为log复制命令，发送通知其他节点完成日志复制请求。

<img src="https://p1-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d62cedc3aace4d4fa2b29f6389544e47~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp" alt="log_replicate.jpg" style="zoom:80%;" />

1. 接收到客户端请求之后，领导者会根据用户指令和自身任期以及日志索引等信息生成一条新的日志项，并附加到本地日志中。
2. 领导者通过日志复制 RPC，将日志复制到其他跟随者节点。
3. 跟随者将日志附加到本地日志成功之后，便返回 success，此时新的日志项还没有被跟随者 apply。
4. 当领导者接收到大多数（超过集群数量的一半）跟随者节点的 success 响应之后，便将此日志应用到它的状态机中。
5. 领导者将执行的结果返回给客户端。
6. 当跟随者收到下一次领导者的心跳请求或者新的日志复制请求之后，如果发现领导者已经应用了之前的日志，但是它自己还没有之后，那么它便会把这条日志项应用到本地状态机中。

在日志复制 RPC 的消息体中，除了会携带当前日志项的信息，还会额外带上当前日志项的前一条日志项的任期编号（PrevLogTerm）和索引值（PrevLogIndex）。跟随者接收到日志复制请求之后，会根据 PrevLogTerm 和 PrevLogIndex 去本地日志查找数据是否存在，如果不存在，返回 fail response。领导者接收到失败返回值之后，会递减要复制的日志索引，直到找到一个共同的任期号&日志索引。此时follower从这个索引值开始复制，最终和leader节点日志保持一致。

**Safety 安全性**

- Election safety: 任一任期内最多一个leader被选出，否者raft的日志复制原则很可能出现数据覆盖丢失的问题。
- Leader append-only: leader只能日志追加日志，不能覆盖或删除日志。
- Log matching: 如果在不同日志中的两个条目有着相同索引和任期号，则它们所存储的命令是相同的，它们之间所有条目完全一样。
- Leader completeness: leader必须拥有最新提交日志。
  - 日志更新判断方式是比较日志项的term和index：如果TermId不同，选择TermId最大的；如果TermId相同，选择Index最大的。
- State machine safety : leader只能提交当前任期Term的日志，旧任期Term（以前的数据）只能通过当前任期Term的数据提交来间接完成提交。

https://juejin.cn/post/6973502080350683149

https://juejin.cn/post/6844903621499305997

https://www.cnblogs.com/xybaby/p/10124083.html

http://dockone.io/article/2434665

#### 一致性哈希

它是指将存储节点和数据都映射到一个首尾相连的哈希环上。这个环的起点是 0，终点是 2^32 - 1，并且起点与终点连接。存储节点一般可以根据 IP 地址或者主机名进行 Hash 计算，数据的存储位置是从数据映射在环上的位置开始，依照顺时针方向所找到的第一个存储节点。

**容错性和可扩展性**

如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间的数据，其它不会受到影响。如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。

因此一致性哈希对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。

**Hash环的数据倾斜问题**

为了解决这种数据倾斜问题，一致性Hash算法引入了**虚拟节点机制**，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器IP或主机名的后面增加编号来实现。

https://cloud.tencent.com/developer/article/1181610

https://segmentfault.com/a/1190000021199728



#### OLTP与OLAP

OLTP（在线事务处理）主要是基本的、日常的事务处理，例如数据库记录的增删改查、银行交易。特点是实时性要求高、数据量不大、高并发，要求满足ACID原则。

OLAP（在线分析处理）是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并提供直观易懂的查询结果，典型的应用就是复杂的动态报表系统。特点是实时性要求不高、数据量大、重点是通过数据提供决策支持，所以查询一般都是动态的、自定义的。

#### 存储引擎

InnoDB

支持事务，采用行锁，支持外键，通过多版本并发控制来获得高并发性，并且实现了SQL标准的4种隔离级别，默认为可重复读，同时，可以使用next-key locking策略来避免幻读的产生。对于表中数据存储，InnoDB采用聚簇索引，每张表的存储都是按主键的顺序进行存放。

MyISAM

不支持事务、采用表锁，支持全文索引。表由MYD和MYI组成，MYD用来存放数据文件，MYI用来存放索引文件。

Memory

将表中的数据存放在内存中，适用于存储临时数据的临时表，默认使用哈希索引。

#### InnoDB关键特性

插入缓冲：对于非聚簇索引的插入或更新操作，不是每次直接插入到索引页中，而是先判断插入的非聚簇索引页是否在缓冲池中，若在，则直接插入；若不在，则先放入insert buffer中。

两次写：在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是会通过memcpy函数将脏页先复制到内存中的double write buffer，之后通过double write buffer再写两次，每次1MB顺序地写入共享表空间地物理磁盘上，然后马上调用fsync函数，同步磁盘。

<img src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7639977c6c004ba59932e12260f0371a~tplv-k3u1fbpfcp-zoom-in-crop-mark:1304:0:0:0.awebp" alt="img" style="zoom: 50%;" />

自适应哈希索引：监控对表上各索引页的查询，如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，即自动根据访问的频率和模式来为某些热点页建立哈希索引。

异步I/O：异步写磁盘

刷新邻接页：在刷新脏页时会检测该页所在区的所有页，如果是脏页，就一起刷新。

#### InnoDB逻辑存储结构

所有数据被逻辑地存放在一个空间中，称之为表空间，表空间又由段（segment）、区（extent，大小为1MB）、页（page，默认大小为16KB）组成。最后，数据按行存放。

全文索引：可以根据需要来查找存储于数据库中整本书或整篇文章中的任意内容信息查找出来。











